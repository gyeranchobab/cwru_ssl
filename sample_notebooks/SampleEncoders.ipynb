{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Various Encoder Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Toy Sample X,Y\n",
    "SAMPLE_X = torch.rand(2, 1, 32)\n",
    "SAMPLE_Y = torch.randint(2, [2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Cute Model\n",
    "    : Tutorial에서 사용했던 SimpleModel의 귀여운 버젼\n",
    "    \n",
    "- 여러 층의 1d Convolution block으로 encode\n",
    "- 여러 층의 Fully-connected Layer로 decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CuteModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CuteModel, self).__init__()\n",
    "        \n",
    "        ## encoder layers : Sequence of Convolution Blocks(Conv1d + ReLU + Dropout)\n",
    "        self.enc_layer1 = nn.Conv1d(1, 8, kernel_size=5, stride=2)\n",
    "        self.enc_layer2 = nn.Conv1d(8, 8, kernel_size=5, stride=2)\n",
    "        \n",
    "        ## classifier layers : Multi-Layer Perceptron(FC + ReLU)\n",
    "        self.decoder = nn.Linear(8, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('Input size :\\t\\t',x.size())\n",
    "        \n",
    "        x = self.enc_layer1(x)\n",
    "        print('After 1st layer :\\t',x.size())\n",
    "        x = self.enc_layer2(x)\n",
    "        print('After 2nd layer :\\t',x.size())\n",
    "        \n",
    "        x = x.mean(dim=-1)\n",
    "        print('After mean pool :\\t',x.size())\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        print('After decoder :\\t\\t',x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size :\t\t torch.Size([2, 1, 32])\n",
      "After 1st layer :\t torch.Size([2, 8, 14])\n",
      "After 2nd layer :\t torch.Size([2, 8, 5])\n",
      "After mean pool :\t torch.Size([2, 8])\n",
      "After decoder :\t\t torch.Size([2, 6])\n",
      "[loss] :  tensor(1.8711, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = CuteModel()\n",
    "p = model(SAMPLE_X)\n",
    "loss = nn.CrossEntropyLoss()(p, SAMPLE_Y)\n",
    "print('[loss] : ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. DilatedCNN\n",
    "< WaveNet >\n",
    "<img src=\"../resources/WaveNet.png\" width=\"500\">\n",
    "\n",
    "- dilated CNN을 통해 input의 주기적 특성을 파악할 수 있습니다.\n",
    "- 각 *self.enc_layer#*을 정의할 때, nn.Conv1d의 인자로 dilation을 정의할 수 있습니다. (기본 : 1)\n",
    "\n",
    "(ex)\n",
    "<pre>\n",
    "self.enc_layer1 = nn.Conv1d(1, 8, kernel_size=5, stride=3, <b>dilation=2</b>)\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DilatedCNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DilatedCNNModel, self).__init__()\n",
    "        \n",
    "        ## encoder layers : Sequence of Convolution Blocks(Conv1d + ReLU + Dropout)\n",
    "        ## you can define dilation here\n",
    "        self.enc_layer1 = nn.Conv1d(1, 8, kernel_size=5, stride=2, dilation=2)\n",
    "        self.enc_layer2 = nn.Conv1d(8, 8, kernel_size=5, stride=2, dilation=2)\n",
    "        \n",
    "        ## classifier layers : Multi-Layer Perceptron(FC + ReLU)\n",
    "        self.decoder = nn.Linear(8, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('Input size :\\t\\t',x.size())\n",
    "\n",
    "        x = self.enc_layer1(x)\n",
    "        print('After 1st layer :\\t',x.size())\n",
    "        x = self.enc_layer2(x)\n",
    "        print('After 2nd layer :\\t',x.size())\n",
    "        \n",
    "        x = x.mean(dim=-1)\n",
    "        print('After mean pool :\\t',x.size())\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        print('After decoder :\\t\\t',x.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size :\t\t torch.Size([2, 1, 32])\n",
      "After 1st layer :\t torch.Size([2, 8, 12])\n",
      "After 2nd layer :\t torch.Size([2, 8, 2])\n",
      "After mean pool :\t torch.Size([2, 8])\n",
      "After decoder :\t\t torch.Size([2, 6])\n",
      "[loss] :  tensor(1.6990, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = DilatedCNNModel()\n",
    "p = model(SAMPLE_X)\n",
    "loss = nn.CrossEntropyLoss()(p, SAMPLE_Y)\n",
    "print('[loss] : ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Cumulate each layer outputs\n",
    "\n",
    "<img src='../resources/cumulate.png' width='600'>\n",
    "\n",
    "- 각 인코딩 레이어의 output을 concatenate함으로써, 고주파 특성과 저주파 특성을 동시에 반영할 수 있습니다.\n",
    "\n",
    "(ex)\n",
    "<pre>\n",
    "x = torch.cat([x1,x2], dim=-1)\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CumulateModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CumulateModel, self).__init__()\n",
    "        \n",
    "        ## encoder layers : Sequence of Convolution Blocks(Conv1d + ReLU + Dropout)\n",
    "        self.enc_layer1 = nn.Conv1d(1, 8, kernel_size=5, stride=3)\n",
    "        self.enc_layer2 = nn.Conv1d(8, 8, kernel_size=5, stride=3)\n",
    "        \n",
    "        ## classifier layers : Multi-Layer Perceptron(FC + ReLU)\n",
    "        self.decoder = nn.Linear(2*8, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('Input size :\\t\\t',x.size())\n",
    "\n",
    "        x1 = self.enc_layer1(x)\n",
    "        print('After 1st layer :\\t',x1.size())\n",
    "        x2 = self.enc_layer2(x1)\n",
    "        print('After 2nd layer :\\t',x2.size())\n",
    "        \n",
    "        ## average pool each layer outputs\n",
    "        x1 = x1.mean(dim=-1)\n",
    "        print('After pool x1 :\\t\\t',x1.size())\n",
    "        x2 = x2.mean(dim=-1)\n",
    "        print('After pool x2 :\\t\\t',x2.size())\n",
    "        \n",
    "        ## concatenate\n",
    "        x = torch.cat([x1,x2], dim=-1)\n",
    "        print('After concat :\\t\\t', x.size())\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        print('After decoder :\\t\\t',x2.size())\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size :\t\t torch.Size([2, 1, 32])\n",
      "After 1st layer :\t torch.Size([2, 8, 10])\n",
      "After 2nd layer :\t torch.Size([2, 8, 2])\n",
      "After pool x1 :\t\t torch.Size([2, 8])\n",
      "After pool x2 :\t\t torch.Size([2, 8])\n",
      "After concat :\t\t torch.Size([2, 16])\n",
      "After decoder :\t\t torch.Size([2, 8])\n",
      "[loss] :  tensor(1.9182, grad_fn=<NllLossBackward>)\n"
     ]
    }
   ],
   "source": [
    "model = CumulateModel()\n",
    "p = model(SAMPLE_X)\n",
    "loss = nn.CrossEntropyLoss()(p, SAMPLE_Y)\n",
    "print('[loss] : ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Variational Model\n",
    "\n",
    "<img src='../resources/variational.png' width='600'>\n",
    "\n",
    "- *z* 분포의 **mean** 과 **log variance**를 계산할 수 있습니다.\n",
    "<pre>\n",
    "z_mu = self.enc_mu(x)\n",
    "z_logvar = self.enc_logvar(x)\n",
    "</pre>\n",
    "\n",
    "- *z_mu*와 *z_logvar*로부터, **reparameterization trick**을 통해 *z*를 샘플링 할 수 있습니다.\n",
    "<pre>\n",
    "z = self._reparam(z_mu, z_logvar)\n",
    "</pre>\n",
    "\n",
    "- z 분포를 Prior 분포인 N(0,1)과 유사하도록 학습하기 위해, **KL-Divergence**를 낮추도록 학습합니다.\n",
    "<pre>\n",
    "self.kld = self._kld_gauss(z_mu, z_logvar)\n",
    "</pre>\n",
    "\n",
    "- **reparameterization trick**과 **KL-Divergence** 계산을 위한 함수는 걱정말고 샘플코드를 가져다 쓰세요!\n",
    "<pre>\n",
    "def _reparam(self, mu, logvar):\n",
    "def _kld_gauss(self, mu, logvar):\n",
    "</pre>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(VariationalModel, self).__init__()\n",
    "        \n",
    "        ## encoder layers : Sequence of Convolution Blocks(Conv1d + ReLU + Dropout)\n",
    "        self.enc_layer1 = nn.Conv1d(1, 8, kernel_size=5, stride=3)\n",
    "        self.enc_layer2 = nn.Conv1d(8, 8, kernel_size=5, stride=3)\n",
    "        \n",
    "        self.enc_mu = nn.Linear(8, 8)\n",
    "        self.enc_logvar = nn.Linear(8, 8)\n",
    "        \n",
    "        self.kld = torch.tensor(0)\n",
    "        \n",
    "        ## classifier layers : Multi-Layer Perceptron(FC + ReLU)\n",
    "        self.decoder = nn.Linear(8, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        print('Input size :\\t\\t',x.size())\n",
    "        \n",
    "        x = self.enc_layer1(x)\n",
    "        print('After 1st layer :\\t',x.size())\n",
    "        x = self.enc_layer2(x)\n",
    "        print('After 2nd layer :\\t',x.size())\n",
    "        \n",
    "        x = x.mean(dim=-1)\n",
    "        print('After mean pool :\\t',x.size())\n",
    "        \n",
    "        z_mu = self.enc_mu(x)\n",
    "        z_logvar = self.enc_logvar(x)\n",
    "        print('z_mu & z_logvar :\\t',z_mu.size())\n",
    "        z = self._reparam(z_mu, z_logvar)\n",
    "        print('Sampled z shape :\\t',z.size())\n",
    "        \n",
    "        self.kld = self._kld_gauss(z_mu, z_logvar)\n",
    "        \n",
    "        x = self.decoder(z)\n",
    "        print('After decoder :\\t\\t',x.size())\n",
    "        return x\n",
    "    \n",
    "    def _reparam(self, mu, logvar):\n",
    "        if self.training:\n",
    "            std = logvar.mul(0.5).exp_()\n",
    "            eps = torch.autograd.Variable(std.data.new(std.size()).normal_())\n",
    "            return eps.mul(std).add_(mu)\n",
    "        else:\n",
    "            return mu\n",
    "        \n",
    "    def _kld_gauss(self, mu, logvar):\n",
    "        kld_element = -logvar + torch.exp(logvar) + mu**2 - 1\n",
    "        return 0.5 * torch.mean(kld_element)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***(IMPORTANT!!!)*** variational model을 사용할 때, latent distribution을 prior에 근사시키기 위해서는 loss term에 ***KL-Divergence term***을 추가해야 합니다.\n",
    "\n",
    "<pre>\n",
    "...\n",
    "class_loss = CrossEntropyLoss()(pred, target)\n",
    "<b>kld_loss = model.kld</b>\n",
    "total_loss = class_loss + <b>BETA*kld_loss</b>\n",
    "total_loss.backward()\n",
    "...\n",
    "</pre>\n",
    "\n",
    "또한, BETA를 통해 각 loss term의 비중을 조절할 수 있습니다. (ex: BETA=0.1 or BETA=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input size :\t\t torch.Size([2, 1, 32])\n",
      "After 1st layer :\t torch.Size([2, 8, 10])\n",
      "After 2nd layer :\t torch.Size([2, 8, 2])\n",
      "After mean pool :\t torch.Size([2, 8])\n",
      "z_mu & z_logvar :\t torch.Size([2, 8])\n",
      "Sampled z shape :\t torch.Size([2, 8])\n",
      "After decoder :\t\t torch.Size([2, 6])\n",
      "[loss] :  tensor(1.6636, grad_fn=<AddBackward0>)\n"
     ]
    }
   ],
   "source": [
    "BETA = 1\n",
    "model = VariationalModel()\n",
    "p = model(SAMPLE_X)\n",
    "loss = nn.CrossEntropyLoss()(p, SAMPLE_Y) + BETA*model.kld\n",
    "print('[loss] : ', loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Try build your own model architecture!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
