{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sample of Semi-supervised Learning Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Toy Sample X,Y, MASK\n",
    "SAMPLE_X = torch.rand(8, 1, 32)\n",
    "SAMPLE_Y = torch.randint(2, [8])\n",
    "EPOCH = 20\n",
    "MASK = torch.tensor([True, True, False, False, False, False, False, False])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CuteModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CuteModel, self).__init__()\n",
    "        \n",
    "        ## encoder layers : Sequence of Convolution Blocks(Conv1d + ReLU + Dropout)\n",
    "        self.enc_layer1 = nn.Conv1d(1, 8, kernel_size=5, stride=2)\n",
    "        self.enc_layer2 = nn.Conv1d(8, 8, kernel_size=5, stride=2)\n",
    "        \n",
    "        ## classifier layers : Multi-Layer Perceptron(FC + ReLU)\n",
    "        self.decoder = nn.Linear(8, 6)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.enc_layer1(x)\n",
    "        x = self.enc_layer2(x)\n",
    "        \n",
    "        x = x.mean(dim=-1)\n",
    "        \n",
    "        x = self.decoder(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Pseudo-Labeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([False, False,  True,  True,  True,  True,  True,  True])\n",
      "tensor([False, False,  True,  True,  True,  True,  True,  True])\n",
      "tensor([False, False,  True,  True,  True,  True,  True,  True])\n",
      "tensor([False, False,  True,  True,  True,  True,  True,  True])\n",
      "tensor([False, False,  True,  True,  True, False,  True,  True])\n",
      "tensor([False, False,  True,  True, False, False,  True,  True])\n",
      "tensor([False, False,  True,  True, False, False,  True,  True])\n",
      "tensor([False, False, False,  True, False, False, False,  True])\n",
      "tensor([False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False])\n",
      "tensor([False, False, False, False, False, False, False, False])\n"
     ]
    }
   ],
   "source": [
    "model = CuteModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "THRES = 0.25  # this is very low value for tutorial. You should use higher value(0.7~)\n",
    "BETA = 0.5\n",
    "\n",
    "for e in range(EPOCH):\n",
    "    p = model(SAMPLE_X)\n",
    "    pred = torch.argmax(p, dim=1)\n",
    "    \n",
    "    prob = F.softmax(p, dim=-1)\n",
    "    maxprob = prob.max(dim=-1)[0]\n",
    "    PSEUDO = (~MASK) & (maxprob>THRES)\n",
    "    print(PSEUDO)\n",
    "    \n",
    "    loss = torch.tensor(0.0).to(device=SAMPLE_X.device)\n",
    "    if MASK.any():\n",
    "        loss += nn.CrossEntropyLoss()(p[MASK], SAMPLE_Y[MASK])\n",
    "    if PSEUDO.any():\n",
    "        pseudo_loss = nn.CrossEntropyLoss(reduction='none')(p[PSEUDO], pred[PSEUDO])\n",
    "        loss += BETA*pseudo_loss.mean()\n",
    "    if MASK.any() or PSEUDO.any():\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Entropy Minimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def HLoss(x):\n",
    "    b = F.softmax(x, dim=1) * F.log_softmax(x, dim=1)\n",
    "    b = -1.0 * b.mean()\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = CuteModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "BETA = 10\n",
    "\n",
    "for e in range(EPOCH):\n",
    "    p = model(SAMPLE_X)\n",
    "    pred = torch.argmax(p, dim=1)\n",
    "    \n",
    "    class_loss = nn.CrossEntropyLoss()(p[MASK], SAMPLE_Y[MASK])\n",
    "    entropy_loss = HLoss(p[~MASK])\n",
    "    \n",
    "    loss = class_loss + BETA*entropy_loss\n",
    "    print('[total] : %.5f,\\t[CrossEntropy] : %.5f,\\t[Entropy] : %.5f'%(loss.item(), class_loss.item(), entropy_loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Consistency Regularization\n",
    "you can give perturbation by temporal shift, adding noise, adversarial training, etc.\n",
    "It is important that the perturbation should be realistic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CrossEntropy(p,q):\n",
    "    b = F.softmax(p, dim=1) * F.log_softmax(q, dim=1)\n",
    "    b = -1.0*b.mean()\n",
    "    return b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CuteModel()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "BETA = 500\n",
    "\n",
    "for e in range(EPOCH):\n",
    "    X_original = SAMPLE_X[:,:,:-1]  # here only used the half of the input\n",
    "    X_perturbed = SAMPLE_X[:,:,1:]  # perturbation : shifting\n",
    "    \n",
    "    p_original = model(X_original)\n",
    "    p_perturbed = model(X_perturbed)\n",
    "    \n",
    "    class_loss = nn.CrossEntropyLoss()(p_original[MASK], SAMPLE_Y[MASK])\n",
    "    dist_loss = CrossEntropy(p_original, p_perturbed)\n",
    "    \n",
    "    loss = class_loss + BETA*dist_loss\n",
    "    print('[total] : %.5f,\\t[CrossEntropy] : %.5f,\\t[Distance] : %.5f'%(loss.item(), class_loss.item(), dist_loss.item()))\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
